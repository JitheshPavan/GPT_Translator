# GPT_Translator

The most innovative concept introduced in Transformers was Attention. What is attention?  
Attention is used to capture the relations between different tokens or words in sequence. For example: "How are you?". Here, there are three tokens (3 words). Attention calculates the relations between these three tokens. Doing so can train it for different tasks, such as text generation. Attention works as follows.  

1) The input to attention mechanism is [Tokens, Embeddings]. Here, Tokens are words that were converted into integers. Embeddings are then used to map these tokens into an embedding space. So, each word will have its own representation or point in the embedding space.  
2) Each token there generates three different vectors. Keys represent the information a token contains. Query which represents what the token needs. Value vector, which is used to map the information generated from key-query interaction into relevant tasks.  
3) First, the information is generated using a dot product between queries and keys. Suppose we have Queries = [Q1, Q2, Q3], where Q1 represents the Query vector generated by token1 and Keys = [K1, K2, K3], then  
    Q. K.transpose. gives = [[Q1*K1, Q1*K2, Q1*K3], [Q2*K1, Q2*K2, Q2*K3], [Q3*K1, Q3*K2, Q3*K3]]. Each row represents the interaction between a query and all of the keys. If we apply a softmax function to each row, we are left with probabilities that sum up to 1. Thus, we are left with a matrix with information about each word's importance in the sequence of a token.  

**Note:**  
- If we are to do a Keys * query, we will have information about a token for others token. The structure of both keys and queries is the same. Thus, both of these formulations are correct. Which matrix functions as the key and query is determined by which side the value token is multiplied from.  
- We do K.T because it gives an inner product between tokens ==> [T, Q] * [K, T] ==> [T, T]. So, the dimensions of the query and keys should be the same.  
- These multiplication works irrespective of the length of the sequence (T). Thus, the attention mechanism can have an infinite context.  

4) softmax(Q.dot(K.Transpose())) * V. What does this do? [T, T] * [T, Embedding_dim] ==> [T, embedding_dim]. We have the original input dimensions back.  
   softmax([[Q1*K1, Q1*K2, Q1*K3]]) * ([[V1], [V2], [V3]]) = [V1[1]*p(Q1*K1) + V2[1]*p(Q1*K2) + V3[1]*p(Q1*K3), V1[2]*p(Q1*K1) + V2[2]*p(Q1*K2) + V3[2]*p(Q1*K3), ...]  
   Thus, we end up with a convex linear combination of individual scalars of values vector.  

- How do different sequence lengths come into the picture? One sequence functions as the query, and one as the keys. [T1,E] * [E,T2] ==> [T1,T2]. Then [T1,T2] * [T2,E] ==> [T1,E]. Thus, the T1 is preserved. To preserve the dimension of output, the values and keys matrix are provided by the sequence, which acts as the context or encoder rather than the decoder.  
- Why individual scalar values of vector? Why not a combination of the values vector itself? Such a combination is incapable of accessing the whole embedding space. For if there are two tokens, it can access a plane only (called column space of the matrix).  
- Is not the combination of every dimension with the same weights undesirable? Yes.  
- Why have a values vector? Why not do the linear combination of the vectors itself? Attention * X instead of V?  
