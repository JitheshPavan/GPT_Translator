{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "9osojLLiDzrf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WeCeACR-8l-"
      },
      "source": [
        "## MHA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obinSauKrskH"
      },
      "source": [
        " It contains 1) Creation of Q,K,V. 2) subsequent division into n_heads by cutting up dimensionality of model 3) self attention in each head. 4) Then combining the heads back 5) convertion of the model into dimensionality of the model using linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "psAR0x64lQHm"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, h, d_k,d_v,d_model):\n",
        "    super().__init__()\n",
        "    self.heads= h\n",
        "    self.d_k = d_k # dimensionality of k,q\n",
        "    self.d_v = d_v # dimensionality of Values\n",
        "    self.d_model = d_model # Model dimension ( 512)\n",
        "    self.W_q  = nn.Linear(d_model, d_k) #\n",
        "    self.W_k  = nn.Linear(d_model, d_k)\n",
        "    self.W_v  = nn.Linear(d_model, d_v)\n",
        "    self.W_o  = nn.Linear(d_v, d_model)\n",
        "    # Mask matrix. We make it untrainable. Register buffer will save this matrix too during torch.save()\n",
        "    # self.register_buffer('tril', torch.tril(torch.ones(max_length, max_length)))\n",
        "\n",
        "  def forward(self, queries, keys, values,mask=None):\n",
        "    #1\n",
        "    q= self.W_q(queries)\n",
        "    k= self.W_k(keys)\n",
        "    v= self.W_v(values)\n",
        "    #2\n",
        "    q= self.reshape_tensor(q)\n",
        "    k= self.reshape_tensor(k)\n",
        "    v= self.reshape_tensor(v)\n",
        "    #3\n",
        "    wei = self.attention(q,k,v,self.d_k,mask)\n",
        "    #4\n",
        "    wei = self.reshape_tensor(wei,reverse=True)\n",
        "    #5\n",
        "    return self.W_o(wei)\n",
        "  # We are transforming the linearly transformed into mutli heads. so that you end up with b,h,t,head//2. So you have transpose. not change the dimensionality of matrix.\n",
        "  # We have done for every batch h heads by cuttting up the embed dimension not the time dimension.\n",
        "  def reshape_tensor(self,x,reverse=False):\n",
        "    if not reverse:\n",
        "      b,t,c = x.size()\n",
        "      return x.view(b,t,self.heads,c//self.heads).transpose(1,2)\n",
        "    else:\n",
        "      x= x.transpose(1,2)\n",
        "      return x.contiguous().view(x.shape[0],x.shape[1],self.d_v)\n",
        "# Attention mechanism\n",
        "  def attention(self, q,k,v,d_k,mask=None):\n",
        "    wei = q @ k.transpose(-2,-1) * d_k**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "    if mask is not None:\n",
        "      wei += -1e9 *mask\n",
        "    wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "    return wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk3w2HWksMxa"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QWWySJCryg4"
      },
      "source": [
        "#### FFNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY4Z79N-6THb"
      },
      "source": [
        "I am going to implement the layernorm and FFNN layer. Layernorm is applied at the end of each sublayer.  FFNN is applied at the end of each encoder and decoder block. FFNN layer is composed of two linear opeartions. It maps to d_ff dim then takes it back. It has a ReLu layer inbetween, not after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DciGOy9M6SDi"
      },
      "outputs": [],
      "source": [
        "class Feed_Forward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "    nn.Linear(d_model, 4 * d_model),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4 * d_model,d_model),\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "class layernorm(nn.Module):\n",
        "  def __init__(self,d_model):\n",
        "    super().__init__()\n",
        "    self.ln=nn.LayerNorm(d_model) # d_model dim is consisten through layers. It fascialltes residual connection as given in forward\n",
        "  def forward(self,x): # Takes in as output the residual connection as well the output of the sublayer.( attention or FFNN) # This is no longer valid for Pre-LN Transformer\n",
        "    return self.ln(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHA96f-NwFpC"
      },
      "source": [
        "#### Encoder Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_I0TECgvTbK"
      },
      "source": [
        "We also introduce a padding mask, which sets all padded tokens to -infinity. This is similar ot look ahead mask introduced in MHA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "s-DJYSLvsL7_"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, model_dimension,expected_max_sequence_length):\n",
        "      super().__init__()\n",
        "      position_id = torch.arange(0, expected_max_sequence_length).unsqueeze(1)\n",
        "      frequencies = torch.pow(10000., -torch.arange(0, model_dimension, 2, dtype=torch.float) / model_dimension)\n",
        "\n",
        "      positional_encodings_table = torch.zeros(expected_max_sequence_length, model_dimension)\n",
        "      positional_encodings_table[:, 0::2] = torch.sin(position_id * frequencies)  # sine on even positions\n",
        "      positional_encodings_table[:, 1::2] = torch.cos(position_id * frequencies)  # cosine on odd positions\n",
        "      self.register_buffer('positional_encodings_table', positional_encodings_table)\n",
        "\n",
        "  def forward(self, embeddings_batch):\n",
        "      assert embeddings_batch.ndim == 3 and embeddings_batch.shape[-1] == self.positional_encodings_table.shape[1], \\\n",
        "          f'Expected (batch size, max token sequence length, model dimension) got {embeddings_batch.shape}'\n",
        "\n",
        "      positional_encodings = self.positional_encodings_table[:embeddings_batch.shape[1]]\n",
        "      return  positional_encodings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3YIqi6tR9OyW"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self,h,d_k,d_v,d_model,rate):\n",
        "    super().__init__()\n",
        "    self.MHA = MultiHeadAttention(h,d_k,d_v,d_model)\n",
        "    self.dropout1= nn.Dropout(rate)\n",
        "    self.layernorm1= layernorm(d_model)\n",
        "    self.Feed_Forward= Feed_Forward(d_model,d_ff)\n",
        "    self.layernorm2= layernorm(d_model)\n",
        "    self.dropout2 = nn.Dropout(rate)\n",
        "  def forward(self,x,padding_mask=None):\n",
        "    x_sideline= self.layernorm1(x)\n",
        "    x = x+ self.dropout1(self.MHA(x_sideline,x_sideline,x_sideline,padding_mask))\n",
        "    x_sideline1= self.layernorm2(x) # mha->dropout-> residual -> layer\n",
        "    x = x+ self.dropout2(self.Feed_Forward(x_sideline1))  #FFNN->dropout-> residual -> layer\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSM5fsG33Hdu"
      },
      "source": [
        "Add a dropout layer after positionalencoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "4OVFAx9E2RzP"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,vocab_size,max_length,d_model,h,d_k,d_v,d_ff, n_layers,rate):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
        "    self.dropout = nn.Dropout(rate)\n",
        "    self.layers = nn.ModuleList([EncoderBlock(h,d_k,d_v,d_model,rate) for _ in range(n_layers)])\n",
        "\n",
        "  def forward(self,sentence, padding_mask):\n",
        "    x = self.embedding(sentence)\n",
        "    x = x + self.positional_encoding(x)\n",
        "    x = self.dropout(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, padding_mask)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erroxYiUIA8_"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TH26YYMQObu"
      },
      "source": [
        "So, the structure goes as below.\n",
        " MHA -> dropout-> residual norm\n",
        " MHA2(Cross attention) -> dropout -> residual norm\n",
        " FF -> dropout -> residual norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "FFhvCrVDI0ni"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self,h,d_k,d_v,d_model,d_ff,rate):\n",
        "    super().__init__()\n",
        "    self.MHA = MultiHeadAttention(h,d_k,d_v,d_model)\n",
        "    self.dropout1= nn.Dropout(rate)\n",
        "    self.add_norm = layernorm(d_model)\n",
        "    self.MHA2 = MultiHeadAttention(h,d_k,d_v,d_model)\n",
        "    self.dropout2= nn.Dropout(rate)\n",
        "    self.add_norm2 = layernorm(d_model)\n",
        "    self.Feed_Forward= Feed_Forward(d_model,d_ff)\n",
        "    self.dropout3= nn.Dropout(rate)\n",
        "    self.add_norm3 = layernorm(d_model)\n",
        "\n",
        "  def forward(self,x,encoder_output,padding_mask=None,look_ahead_mask=None):\n",
        "    x_sideline= self.add_norm(x)\n",
        "    x = x+ self.dropout1(self.MHA(x_sideline,x_sideline,x_sideline,look_ahead_mask))\n",
        "    x_sideline1= self.add_norm2(x)\n",
        "    x = x+ self.dropout2(self.MHA2(x_sideline1,encoder_output,encoder_output,padding_mask)) # Enocoder output because cross attention. Padding mask because keys and values are from encoder. We do not use look_ahead_mask.\n",
        "    x_sideline2= self.add_norm3(x)\n",
        "    x = x+ self.dropout3(self.Feed_Forward(x_sideline2))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "czSA5NY6P8fL"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,vocab_size,max_length, d_model,h,d_k,d_v,d_ff,n_layers,rate):\n",
        "    super().__init__()\n",
        "    self.embedding= nn.Embedding(vocab_size,d_model)\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
        "    self.dropout = nn.Dropout(rate)\n",
        "    self.layers = nn.ModuleList([DecoderBlock(h,d_k,d_v,d_model,d_ff,rate) for _ in range(n_layers)])\n",
        "  def forward(self,decoder_input,encoder_output,lookahead_mask,padding_mask):\n",
        "    x = self.embedding(decoder_input)\n",
        "    x = x + self.positional_encoding(x)\n",
        "    x = self.dropout(x)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x,encoder_output,padding_mask,lookahead_mask)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYe_oWOCR7MI"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W0f2INpoO5eW"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,enc_vocab_size,dec_vocab_size,enc_seq_len,dec_seq_len,d_model,h,d_k,d_v,d_ff,n_layers,rate):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(enc_vocab_size,enc_seq_len,d_model,h,d_k,d_v,d_ff, n_layers,rate)\n",
        "    self.decoder = Decoder(dec_vocab_size,dec_seq_len,d_model,h,d_k,d_v,d_ff,n_layers,rate)\n",
        "    self.model_output = nn.Linear(d_model,dec_vocab_size) #converting d_model to dec_vocab_size)\n",
        "    self.init_weights()\n",
        "  def init_weights(self):\n",
        "    for p in self.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "  def forward(self,encoder_input,decoder_input):\n",
        "    padding_mask_enc   = self.padding_mask(encoder_input)\n",
        "    padding_mask_dec   = self.padding_mask(decoder_input)\n",
        "    lookahead_mask_dec = self.look_ahead_mask(decoder_input.shape[1])\n",
        "    lookahead_mask_dec = torch.maximum(lookahead_mask_dec,padding_mask_dec) # important. Here we are using look_ahead_mask and padding_mask. Padding Mask is in relation with the decoder input. not the encoder input.\n",
        "    encoder_output     = self.encoder(encoder_input,padding_mask_enc)\n",
        "    decoder_output     = self.decoder(decoder_input,encoder_output,lookahead_mask_dec,padding_mask_enc)\n",
        "    return self.model_output(decoder_output)\n",
        "  def padding_mask(self,x): # Changes the values to 1 from 0. Where zero is pad vocab\n",
        "    x=(x==0).float()\n",
        "    x=x.type(torch.float32)\n",
        "    return x[:,None,None,:]\n",
        "\n",
        "  def look_ahead_mask(self,shape):\n",
        "    x=torch.triu(torch.ones((shape,shape)),diagonal=1)\n",
        "    return x.type(torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instance"
      ],
      "metadata": {
        "id": "iBfqRlXpEA8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "onjaHAoMEQdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_vocab_size=100\n",
        "dec_vocab_size=100\n",
        "enc_seq_len=10\n",
        "dec_seq_len=10"
      ],
      "metadata": {
        "id": "btKdfQKJEU7T"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= torch.randint(0,enc_vocab_size,(1,enc_seq_len))\n",
        "y= torch.randint(0,dec_vocab_size,(1,dec_seq_len))"
      ],
      "metadata": {
        "id": "5Xz-8RaoEP-F"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n"
      ],
      "metadata": {
        "id": "sxUzIT0PEqf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h= 8 # number of heads in a MHA block\n",
        "d_model=512\n",
        "d_k= 512\n",
        "d_v= 512\n",
        "d_ff= 2048\n",
        "n_layers= 6\n",
        "rate= 0.1"
      ],
      "metadata": {
        "id": "f2-5mOHqEsI5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer(enc_vocab_size,dec_vocab_size,enc_seq_len,dec_seq_len,d_model,h,d_k,d_v,d_ff,n_layers,rate)"
      ],
      "metadata": {
        "id": "6hiZWQX3EAZz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_input= x\n",
        "dec_input= y[:,:-1]\n",
        "target= y[:,1:] # you do not predict the first token, because it is the <START> token.\n",
        "result=model.forward(enc_input,dec_input)\n",
        "print(f'shape of the output={result.shape}') # You can use argmax to get the output. This is the output token predicted considering the input tokens at and before its position."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4G_R_zBFhf7",
        "outputId": "6b855f61-64a6-4685-ef74-e5d5ac3a830d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the output=torch.Size([1, 9, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l3JrQo8FFs0u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}