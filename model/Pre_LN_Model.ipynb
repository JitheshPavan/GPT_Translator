{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "b0vw0ftzgZDv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model parameters\n",
        "h = 8  # Number of self-attention heads\n",
        "d_k = 64  # Dimensionality of the linearly projected queries and keys # The dimension is divided among the heads.  Thus every head key value will be d_k/h= 8\n",
        "d_v = 64  # Dimensionality of the linearly projected values\n",
        "d_model = 512  # Dimensionality of model layers' outputs\n",
        "d_ff = 2048  # Dimensionality of the inner fully connected layer # Usually 4* d_model\n",
        "n = 6  # Number of layers in the encoder stack\n",
        "\n",
        "#Training\n",
        "batch_size = 64\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.98\n",
        "epsilon = 1e-9\n",
        "dropout_rate = 0.1"
      ],
      "metadata": {
        "id": "kesq4E3ugpnT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZMYSg0TgeQM"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evJaanoDgeQN"
      },
      "source": [
        "## Multi Head Attention (MHA):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPJ29DsLgeQN"
      },
      "source": [
        " Steps in MHA</br>\n",
        "\n",
        " 1) Transformation into Q,K,V matrices. </br>\n",
        "\n",
        " 2) Subsequent division of last dimension( embedding dimension) into n_heads\n",
        "\n",
        " 3) self attention in each head. Note: Can be computed in a single matrix mulitplication\n",
        "\n",
        "4) Combining the heads back\n",
        "\n",
        "5) Get final output by mulitplying the value matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qMZcF8i0geQN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, h: int, d_k: int, d_v: int, d_model: int) -> None:\n",
        "    \"\"\"\n",
        "    Initializes the MultiHeadAttention module.\n",
        "\n",
        "    Args:\n",
        "        h (int): Number of attention heads.\n",
        "        d_k (int): Dimensionality of keys and queries.\n",
        "        d_v (int): Dimensionality of values.\n",
        "        d_model (int): Dimensionality of the model.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.heads = h\n",
        "    self.d_k = d_k\n",
        "    self.d_v = d_v\n",
        "    self.d_model = d_model\n",
        "\n",
        "    # Linear transformations for queries, keys, values, and the output\n",
        "    self.W_q = nn.Linear(d_model, d_k)\n",
        "    self.W_k = nn.Linear(d_model, d_k)\n",
        "    self.W_v = nn.Linear(d_model, d_v)\n",
        "    self.W_o = nn.Linear(d_v, d_model)\n",
        "\n",
        "  def forward( self,  queries: torch.Tensor, keys: torch.Tensor,\n",
        "    values: torch.Tensor,  mask: Optional[torch.Tensor] = None ) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Performs the forward pass of the multi-head attention mechanism.\n",
        "\n",
        "    Args:\n",
        "        queries (torch.Tensor): Query tensor of shape (batch_size, seq_length, d_model).\n",
        "        keys (torch.Tensor): Key tensor of shape (batch_size, seq_length, d_model).\n",
        "        values (torch.Tensor): Value tensor of shape (batch_size, seq_length, d_model).\n",
        "        mask (Optional[torch.Tensor]): Mask tensor of shape (batch_size, seq_length, seq_length) or None.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor of shape (batch_size, seq_length, d_model).\n",
        "    \"\"\"\n",
        "    # 1. Linear projections for queries, keys, and values\n",
        "\n",
        "    q = self.W_q(queries)\n",
        "    k = self.W_k(keys)\n",
        "    v = self.W_v(values)\n",
        "\n",
        "    # 2. Reshape into multi-head format\n",
        "    q = self.reshape_tensor(q)\n",
        "    k = self.reshape_tensor(k)\n",
        "    v = self.reshape_tensor(v)\n",
        "    # 3. Compute scaled dot-product attention\n",
        "    wei = self.attention(q, k, v, self.d_k, mask)\n",
        "\n",
        "    # 4. Reshape back to original format\n",
        "\n",
        "    wei = self.reshape_tensor(wei, reverse=True)\n",
        "\n",
        "    # 5. Apply final linear transformation\n",
        "    return self.W_o(wei)\n",
        "\n",
        "  def reshape_tensor(self, x: torch.Tensor, reverse: bool = False) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Reshapes the tensor for multi-head attention computation.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Input tensor of shape (batch_size, seq_length, d_model).\n",
        "        reverse (bool): If True, reshapes back to the original format.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Reshaped tensor.\n",
        "\n",
        "    Note: Transpose has to be applied here- to turn [B,T,H,h/d_k]==> [B,H,T,h/d_k].\n",
        "          Because during the attention two dimensions has to participate, namely time/token dimension and embedding dimension (not the head dimension).\n",
        "          So transpose is necessary here.\n",
        "    \"\"\"\n",
        "    if not reverse:\n",
        "      b, t, c = x.size()\n",
        "      return x.view(b, t, self.heads, c // self.heads).transpose(1, 2)\n",
        "    else:\n",
        "      x = x.transpose(1, 2)\n",
        "      return x.contiguous().view(x.shape[0], x.shape[1], self.d_v)\n",
        "\n",
        "  def attention(self,  q: torch.Tensor, k: torch.Tensor, v: torch.Tensor,d_k: int, mask: Optional[torch.Tensor] = None\n",
        "  ) -> torch.Tensor:\n",
        "      \"\"\"\n",
        "      Computes the scaled dot-product attention.\n",
        "\n",
        "      Args:\n",
        "          q (torch.Tensor): Query tensor of shape (batch_size, heads, seq_length, d_k).\n",
        "          k (torch.Tensor): Key tensor of shape (batch_size, heads, seq_length, d_k).\n",
        "          v (torch.Tensor): Value tensor of shape (batch_size, heads, seq_length, d_v).\n",
        "          d_k (int): Dimensionality of keys and queries.\n",
        "          mask (Optional[torch.Tensor]): Mask tensor of shape (batch_size, seq_length, seq_length) or None.\n",
        "\n",
        "      Returns:\n",
        "          torch.Tensor: Output tensor of shape (batch_size, heads, seq_length, d_v).\n",
        "      \"\"\"\n",
        "      wei = q @ k.transpose(-2, -1) * d_k**-0.5  # (B, H, T, D_k) @ (B, H, D_k, T) -> (B, H, T, T)\n",
        "      if mask is not None:\n",
        "          wei += -1e9 * mask  # Large negative values give zero for softmax,\n",
        "      wei = F.softmax(wei, dim=-1)  # Normalize attention scores\n",
        "      return wei @ v  # (B, H, T, T) @ (B, H, T, D_v) -> (B, H, T, D_v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyBTvXDogeQO"
      },
      "source": [
        "## FFNN and Layernorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRie0S6ZgeQO"
      },
      "source": [
        "\n",
        "FFNN is applied at the end of each encoder and decoder block. FFNN layer is composed of two linear opeartions. This layer changes the last dimension to d_ff an then takes it back. This layer has a ReLU layer inbetween, but not after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N3gtmKOngeQO"
      },
      "outputs": [],
      "source": [
        "class Feed_Forward(nn.Module):\n",
        "  def __init__(self, d_model : int, d_ff: int)-> None:\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(d_model, d_ff),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(d_ff, d_model),\n",
        "    )\n",
        "  def forward(self,x:torch.Tensor)-> torch.Tensor:\n",
        "    return self.net(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layernorm is applied at the end of each sublayer. </br>"
      ],
      "metadata": {
        "id": "vT9goAgHajeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Encoding"
      ],
      "metadata": {
        "id": "j56_r8bIgm3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional Encoding table is precomputed.\n",
        "\n",
        "1) Number of rows is determined by max sequence length (This is extracted from the training set).\n",
        "\n",
        "2) Number of columns is embedding dimension or model_dimension."
      ],
      "metadata": {
        "id": "Wf1pm8G1g9Hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, model_dimension: int, expected_max_sequence_length: int)-> None:\n",
        "    super().__init__()\n",
        "    position_id = torch.arange(expected_max_sequence_length).unsqueeze(1)\n",
        "    frequencies = 10000 ** (-torch.arange(0, model_dimension, 2, dtype=torch.float) / model_dimension)\n",
        "\n",
        "    # Precompute the positional encodings\n",
        "    positional_encodings_table = torch.zeros(expected_max_sequence_length, model_dimension)\n",
        "    positional_encodings_table[:, 0::2] = torch.sin(position_id * frequencies)\n",
        "    positional_encodings_table[:, 1::2] = torch.cos(position_id * frequencies)\n",
        "\n",
        "    # Save the encodings as a non-trainable buffer\n",
        "    self.register_buffer('positional_encodings_table', positional_encodings_table)\n",
        "\n",
        "  def forward(self, embeddings_batch: torch.Tensor)-> torch.Tensor:\n",
        "\n",
        "    assert embeddings_batch.shape[-1] == self.positional_encodings_table.size(1), \\\n",
        "      f\"Model dimension mismatch: {embeddings_batch.shape[-1]} != {self.positional_encodings_table.size(1)}\"\n",
        "\n",
        "    # Select and return positional encodings matching the sequence length\n",
        "    return self.positional_encodings_table[:embeddings_batch.size(1)]\n"
      ],
      "metadata": {
        "id": "vOQiRUXUgkOL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6BEHAulgeQS"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toa-MQrdgeQS"
      },
      "source": [
        "#### Encoder Block"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "x------------------------------------------------------x</br>\n",
        "|&nbsp; &nbsp; &nbsp;    &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;     + &nbsp; &nbsp; =&nbsp; &nbsp; &nbsp; layernorm(x)------------------------------layernorm(x) </br>\n",
        "x->layernorm(x)->MHA(x)->dropout(x)    &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;     |                          &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;   +  = output</br>\n",
        "     &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp;                                       FFNN(x)-- ReLU(x)--FNNN(x) --- dropout(x)"
      ],
      "metadata": {
        "id": "X1kKVN-Uh7Nz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ln3AYITUgeQT"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self, h: int, d_k: int, d_v: int, d_model: int, rate: float, d_ff: int= d_ff) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.MHA = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "\n",
        "    self.dropout1 = nn.Dropout(rate)\n",
        "    self.dropout2 = nn.Dropout(rate)\n",
        "\n",
        "    self.layernorm1 = nn.LayerNorm(d_model)\n",
        "    self.layernorm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.Feed_Forward = Feed_Forward(d_model, d_ff)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, padding_mask: Optional[torch.Tensor] = None ) -> torch.Tensor:\n",
        "\n",
        "    x_sideline = self.layernorm1(x)\n",
        "\n",
        "    x = x + self.dropout1(self.MHA(x_sideline, x_sideline, x_sideline, padding_mask))\n",
        "\n",
        "    x_sideline1 = self.layernorm2(x)  # mha -> dropout -> residual -> layer norm\n",
        "\n",
        "    x = x + self.dropout2(self.Feed_Forward(x_sideline1))  # FFNN -> dropout -> residual -> layer norm\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BpGtL9WgeQT"
      },
      "source": [
        "Encoder block is repeated 8 (n_layers) times.\n",
        "Nt: Dropout layer is added after positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "XQnEIrf9gU6g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZHyjQHM9geQT"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size: int, max_length: int, d_model: int, h: int, d_k: int, d_v: int, d_ff: int, n_layers: int, rate: float) -> None:\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
        "    self.dropout = nn.Dropout(rate)\n",
        "\n",
        "    self.layers = nn.ModuleList([EncoderBlock(h, d_k, d_v, d_model, rate) for _ in range(n_layers)])\n",
        "\n",
        "  def forward(self, sentence: torch.Tensor, padding_mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
        "    x = self.embedding(sentence)\n",
        "    x = x + self.positional_encoding(x)\n",
        "    x = self.dropout(x)\n",
        "    for layer in self.layers:\n",
        "        x = layer(x, padding_mask)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOoLDChageQT"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKcp7rS6geQU"
      },
      "source": [
        "So, the structure goes as below.\n",
        " norm-> MHA -> dropout-> residual norm\n",
        " MHA2(Cross attention) -> dropout -> residual norm\n",
        " FF -> dropout -> residual norm.\n",
        "\n",
        " **Note**:\n",
        "\n",
        " 1) Layer norms are applied before sublayers rather than after.\n",
        "\n",
        " 2) Padding mask is necessary for every attention computation. But, look ahead mask is used only during decoder self attention\n",
        "*   Encoder has sentence which needs to be translated. So encoder should have access to all the tokenns in a sentence.\n",
        "*   Decoder translates the sentence. That is why it should not have access future words during training. That is why look aheed mask is used in a decoder not in encoder.\n",
        "*  The Look ahead mask is not used during cross attention. This is because keys are provided by the encoder and it does not use this mask\n",
        "* The padding mask for cross attention is constructed from the encoder input.\n",
        "\n",
        "In the code look_ahead mask already has the paddin mask engrained with it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "k1UOUorKgeQU"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, h: int, d_k: int, d_v: int, d_model: int, d_ff: int, rate: float) -> None:\n",
        "    super().__init__()\n",
        "    self.MHA = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "    self.MHA2 = MultiHeadAttention(h, d_k, d_v, d_model)\n",
        "\n",
        "    # It is not necessary to define dropout layers mulitple times. It is stateless.\n",
        "    self.dropout3 = nn.Dropout(rate)\n",
        "    self.dropout2 = nn.Dropout(rate)\n",
        "    self.dropout1 = nn.Dropout(rate)\n",
        "\n",
        "    # Layernorm have learned parameters(Gamma and beta)\n",
        "    self.add_norm = nn.LayerNorm(d_model)\n",
        "    self.add_norm2 = nn.LayerNorm(d_model)\n",
        "    self.add_norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.Feed_Forward = Feed_Forward(d_model, d_ff)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, encoder_output: torch.Tensor, padding_mask: Optional[torch.Tensor] = None, look_ahead_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "\n",
        "    x_sideline = self.add_norm(x)\n",
        "    x = x + self.dropout1(self.MHA(x_sideline, x_sideline, x_sideline, look_ahead_mask))\n",
        "\n",
        "    x_sideline1 = self.add_norm2(x)\n",
        "# Encoder output is used because this is cross-attention. Padding mask is constructed from encoder input\n",
        "    x = x + self.dropout2(self.MHA2(x_sideline1, encoder_output, encoder_output, padding_mask))\n",
        "\n",
        "\n",
        "    x_sideline2 = self.add_norm3(x)\n",
        "    x = x + self.dropout3(self.Feed_Forward(x_sideline2))\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "H1Re9A1AgeQU"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size: int, max_length: int, d_model: int, h: int, d_k: int, d_v: int, d_ff: int, n_layers: int, rate: float) -> None:\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    self.positional_encoding = PositionalEncoding(d_model, max_length)\n",
        "    self.dropout = nn.Dropout(rate)\n",
        "\n",
        "    self.layers = nn.ModuleList([DecoderBlock(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n_layers)])\n",
        "\n",
        "  def forward(self, decoder_input: torch.Tensor, encoder_output: torch.Tensor, lookahead_mask: Optional[torch.Tensor], padding_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
        "    x = self.embedding(decoder_input)\n",
        "\n",
        "    x = x + self.positional_encoding(x)\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "\n",
        "        x = layer(x, encoder_output, padding_mask, lookahead_mask)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cW8dgwZkgeQU"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QZRM04LugeQU"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Union, Type\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, enc_vocab_size: int, dec_vocab_size: int, enc_seq_len: int, dec_seq_len: int,\n",
        "               d_model: int, h: int, d_k: int, d_v: int, d_ff: int, n_layers: int, rate: float) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = Encoder(enc_vocab_size, enc_seq_len, d_model, h, d_k, d_v, d_ff, n_layers, rate)\n",
        "    self.decoder = Decoder(dec_vocab_size, dec_seq_len, d_model, h, d_k, d_v, d_ff, n_layers, rate)\n",
        "    self.model_output = nn.Linear(d_model, dec_vocab_size)\n",
        "\n",
        "    self.init_weights() # Xavier Initialization\n",
        "\n",
        "  def forward(self, encoder_input: torch.Tensor, decoder_input: torch.Tensor) -> torch.Tensor:\n",
        "    # Creating padding mask. This will remove padded tokens out of contibution during attention.\n",
        "    padding_mask_enc = self.padding_mask(encoder_input)\n",
        "    padding_mask_dec = self.padding_mask(decoder_input)\n",
        "\n",
        "    lookahead_mask_dec = self.look_ahead_mask(decoder_input.shape[1])\n",
        "    lookahead_mask_dec = torch.maximum(lookahead_mask_dec, padding_mask_dec)\n",
        "\n",
        "    encoder_output = self.encoder(encoder_input, padding_mask_enc)\n",
        "    decoder_output = self.decoder(decoder_input, encoder_output, lookahead_mask_dec, padding_mask_enc)\n",
        "    return self.model_output(decoder_output)\n",
        "\n",
        "\n",
        "  def padding_mask(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    return (x == 0).float().unsqueeze(1).unsqueeze(1) # Shape =[Batch,1,1,seq_length]. This will broadcast during attention to [Batch,heads,seq_length,seq_length].\n",
        "\n",
        "  def look_ahead_mask(self, shape: int) -> torch.Tensor:\n",
        "    x = torch.tril(torch.ones((shape, shape), device=device))\n",
        "    return x.type(torch.float32)\n",
        "\n",
        "  def init_weights(self) -> None:\n",
        "    for p in self.parameters():\n",
        "      if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instance"
      ],
      "metadata": {
        "id": "iBfqRlXpEA8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "onjaHAoMEQdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_vocab_size=100\n",
        "dec_vocab_size=100\n",
        "enc_seq_len=10\n",
        "dec_seq_len=10"
      ],
      "metadata": {
        "id": "btKdfQKJEU7T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= torch.randint(0,enc_vocab_size,(1,enc_seq_len))\n",
        "y= torch.randint(0,dec_vocab_size,(1,dec_seq_len))"
      ],
      "metadata": {
        "id": "5Xz-8RaoEP-F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n"
      ],
      "metadata": {
        "id": "sxUzIT0PEqf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "MBSC-buag_6c"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Transformer(enc_vocab_size,dec_vocab_size,enc_seq_len,dec_seq_len,d_model,h,d_k,d_v,d_ff,n,dropout_rate)"
      ],
      "metadata": {
        "id": "f2-5mOHqEsI5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_input= x\n",
        "dec_input= y[:,:-1]\n",
        "target= y[:,1:] # Taget should be one off input. If the sentence is 'This is GPT'; then input = 'This', output ='is'\n",
        "result=model.forward(enc_input,dec_input)\n",
        "print(f'shape of the output={result.shape}') # You can use argmax to get the output. This is the output token predicted considering the input tokens at and before its position."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4G_R_zBFhf7",
        "outputId": "e3013762-bf70-455d-d0d0-17a57c60f6f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the output=torch.Size([1, 9, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DrA4OSu8hA5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}